{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d274ef",
   "metadata": {},
   "source": [
    "# Model experiments for Animals classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71278345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from abc import abstractmethod\n",
    "from dataclasses import dataclass\n",
    "import PIL\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import decode_image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e1c8e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d559a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishLabel(str, Enum):\n",
    "    DOG = \"dog\"\n",
    "    HORSE = \"horse\"\n",
    "    ELEPHANT = \"elephant\"\n",
    "    BUTTERFLY = \"butterfly\"\n",
    "    CHICKEN = \"chicken\"\n",
    "    CAT = \"cat\"\n",
    "    COW = \"cow\"\n",
    "    SHEEP = \"sheep\"\n",
    "    SPIDER = \"spider\"\n",
    "    SQUIRREL = \"squirrel\"\n",
    "\n",
    "\n",
    "class ItalianLabel(str, Enum):\n",
    "    CANE = \"cane\"\n",
    "    CAVALLO = \"cavallo\"\n",
    "    ELEFANTE = \"elefante\"\n",
    "    FARFALLA = \"farfalla\"\n",
    "    GALLINA = \"gallina\"\n",
    "    GATTO = \"gatto\"\n",
    "    MUCCA = \"mucca\"\n",
    "    PECORA = \"pecora\"\n",
    "    RAGNO = \"ragno\"\n",
    "    SCOIATTOLO = \"scoiattolo\"\n",
    "\n",
    "\n",
    "def translate_labels(labels: list[EnglishLabel]) -> list[ItalianLabel]:\n",
    "    translate = {\n",
    "        EnglishLabel.DOG: ItalianLabel.CANE,\n",
    "        EnglishLabel.HORSE: ItalianLabel.CAVALLO,\n",
    "        EnglishLabel.ELEPHANT: ItalianLabel.ELEFANTE,\n",
    "        EnglishLabel.BUTTERFLY: ItalianLabel.FARFALLA,\n",
    "        EnglishLabel.CHICKEN: ItalianLabel.GALLINA,\n",
    "        EnglishLabel.CAT: ItalianLabel.GATTO,\n",
    "        EnglishLabel.COW: ItalianLabel.MUCCA,\n",
    "        EnglishLabel.SHEEP: ItalianLabel.PECORA,\n",
    "        EnglishLabel.SPIDER: ItalianLabel.RAGNO,\n",
    "        EnglishLabel.SQUIRREL: ItalianLabel.SCOIATTOLO,\n",
    "    }\n",
    "    return [translate[label] for label in labels]\n",
    "\n",
    "\n",
    "def build_dataframes(\n",
    "    data_dir: str | Path, english_labels: list[EnglishLabel], test_size: float = 0.2\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, int]:\n",
    "    data_dir = Path(data_dir)\n",
    "    if not data_dir.exists() or not data_dir.is_dir():\n",
    "        raise ValueError(f\"Invalid data directory: {data_dir}\")\n",
    "\n",
    "    italian_labels = translate_labels(english_labels)\n",
    "    num_labels = len(italian_labels)\n",
    "\n",
    "    data = []\n",
    "    for label_idx, italian_label in enumerate(italian_labels):\n",
    "        label_dir = data_dir / italian_label.value\n",
    "        for fpath in label_dir.glob(\"*\"):\n",
    "            if fpath.is_file():\n",
    "                data.append(\n",
    "                    {\n",
    "                        \"italian_label\": italian_label,\n",
    "                        \"english_label\": english_labels[label_idx],\n",
    "                        \"label_idx\": label_idx,\n",
    "                        \"path\": str(fpath),\n",
    "                    }\n",
    "                )\n",
    "    data_df = pd.DataFrame(data)\n",
    "\n",
    "    train_df, test_df = train_test_split(data_df, test_size=test_size)\n",
    "    return train_df, test_df, num_labels\n",
    "\n",
    "\n",
    "class AnimalsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, data_df: pd.DataFrame, num_labels: int, transform: Any | None = None\n",
    "    ):\n",
    "        self.data_df = data_df\n",
    "        self.num_labels = num_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        entry = self.data_df.iloc[idx]\n",
    "        image = decode_image(entry[\"path\"])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels = np.zeros(self.num_labels, dtype=int)\n",
    "        labels[entry[\"label_idx\"]] = 1\n",
    "        return image, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8c0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, num_labels = build_dataframes(\n",
    "    \"../data/raw-img\", [EnglishLabel.DOG, EnglishLabel.HORSE], test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76103bd8",
   "metadata": {},
   "source": [
    "### SENN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7e88a",
   "metadata": {},
   "source": [
    "#### Conceptizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cce38c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conceptizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        A general Conceptizer meta-class. Children of the Conceptizer class\n",
    "        should implement encode() and decode() functions.\n",
    "        \"\"\"\n",
    "        super(Conceptizer, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the general conceptizer.\n",
    "\n",
    "        Computes concepts present in the input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input data tensor of shape (BATCH, *). Only restriction on the shape is that\n",
    "            the first dimension should correspond to the batch size.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        encoded : torch.Tensor\n",
    "            Encoded concepts (batch_size, concept_number, concept_dimension)\n",
    "        decoded : torch.Tensor\n",
    "            Reconstructed input (batch_size, *)\n",
    "        \"\"\"\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return encoded, decoded.view_as(x)\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Abstract encode function to be overridden.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input data tensor of shape (BATCH, *). Only restriction on the shape is that\n",
    "            the first dimension should correspond to the batch size.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def decode(self, encoded):\n",
    "        \"\"\"\n",
    "        Abstract decode function to be overridden.\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded : torch.Tensor\n",
    "            Latent representation of the data\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class ConvConceptizer(Conceptizer):\n",
    "    def __init__(self, image_size, num_concepts, concept_dim, image_channels=1, encoder_channels=(10,),\n",
    "                 decoder_channels=(16, 8), kernel_size_conv=5, kernel_size_upsample=(5, 5, 2),\n",
    "                 stride_conv=1, stride_pool=2, stride_upsample=(2, 1, 2),\n",
    "                 padding_conv=0, padding_upsample=(0, 0, 1), **kwargs):\n",
    "        \"\"\"\n",
    "        CNN Autoencoder used to learn the concepts, present in an input image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image_size : int\n",
    "            the width of the input image\n",
    "        num_concepts : int\n",
    "            the number of concepts\n",
    "        concept_dim : int\n",
    "            the dimension of each concept to be learned\n",
    "        image_channels : int\n",
    "            the number of channels of the input images\n",
    "        encoder_channels : tuple[int]\n",
    "            a list with the number of channels for the hidden convolutional layers\n",
    "        decoder_channels : tuple[int]\n",
    "            a list with the number of channels for the hidden upsampling layers\n",
    "        kernel_size_conv : int, tuple[int]\n",
    "            the size of the kernels to be used for convolution\n",
    "        kernel_size_upsample : int, tuple[int]\n",
    "            the size of the kernels to be used for upsampling\n",
    "        stride_conv : int, tuple[int]\n",
    "            the stride of the convolutional layers\n",
    "        stride_pool : int, tuple[int]\n",
    "            the stride of the pooling layers\n",
    "        stride_upsample : int, tuple[int]\n",
    "            the stride of the upsampling layers\n",
    "        padding_conv : int, tuple[int]\n",
    "            the padding to be used by the convolutional layers\n",
    "        padding_upsample : int, tuple[int]\n",
    "            the padding to be used by the upsampling layers\n",
    "        \"\"\"\n",
    "        super(ConvConceptizer, self).__init__()\n",
    "        self.num_concepts = num_concepts\n",
    "        self.filter = filter\n",
    "        self.dout = image_size\n",
    "\n",
    "        # Encoder params\n",
    "        encoder_channels = (image_channels,) + encoder_channels\n",
    "        kernel_size_conv = handle_integer_input(kernel_size_conv, len(encoder_channels))\n",
    "        stride_conv = handle_integer_input(stride_conv, len(encoder_channels))\n",
    "        stride_pool = handle_integer_input(stride_pool, len(encoder_channels))\n",
    "        padding_conv = handle_integer_input(padding_conv, len(encoder_channels))\n",
    "        encoder_channels += (num_concepts,)\n",
    "\n",
    "        # Decoder params\n",
    "        decoder_channels = (num_concepts,) + decoder_channels\n",
    "        kernel_size_upsample = handle_integer_input(kernel_size_upsample, len(decoder_channels))\n",
    "        stride_upsample = handle_integer_input(stride_upsample, len(decoder_channels))\n",
    "        padding_upsample = handle_integer_input(padding_upsample, len(decoder_channels))\n",
    "        decoder_channels += (image_channels,)\n",
    "\n",
    "        # Encoder implementation\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(len(encoder_channels) - 1):\n",
    "            self.encoder.append(self.conv_block(in_channels=encoder_channels[i],\n",
    "                                                out_channels=encoder_channels[i + 1],\n",
    "                                                kernel_size=kernel_size_conv[i],\n",
    "                                                stride_conv=stride_conv[i],\n",
    "                                                stride_pool=stride_pool[i],\n",
    "                                                padding=padding_conv[i]))\n",
    "            self.dout = (self.dout - kernel_size_conv[i] + 2 * padding_conv[i] + stride_conv[i] * stride_pool[i]) // (\n",
    "                    stride_conv[i] * stride_pool[i])\n",
    "\n",
    "        if self.filter and concept_dim == 1:\n",
    "            self.encoder.append(ScalarMapping((self.num_concepts, self.dout, self.dout)))\n",
    "        else:\n",
    "            self.encoder.append(Flatten())\n",
    "            self.encoder.append(nn.Linear(self.dout ** 2, concept_dim))\n",
    "\n",
    "        # Decoder implementation\n",
    "        self.unlinear = nn.Linear(concept_dim, self.dout ** 2)\n",
    "        self.decoder = nn.ModuleList()\n",
    "        decoder = []\n",
    "        for i in range(len(decoder_channels) - 1):\n",
    "            decoder.append(self.upsample_block(in_channels=decoder_channels[i],\n",
    "                                               out_channels=decoder_channels[i + 1],\n",
    "                                               kernel_size=kernel_size_upsample[i],\n",
    "                                               stride_deconv=stride_upsample[i],\n",
    "                                               padding=padding_upsample[i]))\n",
    "            decoder.append(nn.ReLU(inplace=True))\n",
    "        decoder.pop()\n",
    "        decoder.append(nn.Tanh())\n",
    "        self.decoder = nn.ModuleList(decoder)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        The encoder part of the autoencoder which takes an Image as an input\n",
    "        and learns its hidden representations (concepts)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Image (batch_size, channels, width, height)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        encoded : torch.Tensor (batch_size, concept_number, concept_dimension)\n",
    "            the concepts representing an image\n",
    "\n",
    "        \"\"\"\n",
    "        encoded = x\n",
    "        for module in self.encoder:\n",
    "            encoded = module(encoded)\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        The decoder part of the autoencoder which takes a hidden representation as an input\n",
    "        and tries to reconstruct the original image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : torch.Tensor (batch_size, channels, width, height)\n",
    "            the concepts in an image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reconst : torch.Tensor (batch_size, channels, width, height)\n",
    "            the reconstructed image\n",
    "\n",
    "        \"\"\"\n",
    "        reconst = self.unlinear(z)\n",
    "        reconst = reconst.view(-1, self.num_concepts, self.dout, self.dout)\n",
    "        for module in self.decoder:\n",
    "            reconst = module(reconst)\n",
    "        return reconst\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels, kernel_size, stride_conv, stride_pool, padding):\n",
    "        \"\"\"\n",
    "        A helper function that constructs a convolution block with pooling and activation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            the number of input channels\n",
    "        out_channels : int\n",
    "            the number of output channels\n",
    "        kernel_size : int\n",
    "            the size of the convolutional kernel\n",
    "        stride_conv : int\n",
    "            the stride of the deconvolution\n",
    "        stride_pool : int\n",
    "            the stride of the pooling layer\n",
    "        padding : int\n",
    "            the size of padding\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sequence : nn.Sequence\n",
    "            a sequence of convolutional, pooling and activation modules\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=kernel_size,\n",
    "                      stride=stride_conv,\n",
    "                      padding=padding),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "            nn.AvgPool2d(kernel_size=stride_pool,\n",
    "                         padding=padding),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upsample_block(self, in_channels, out_channels, kernel_size, stride_deconv, padding):\n",
    "        \"\"\"\n",
    "        A helper function that constructs an upsampling block with activations\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            the number of input channels\n",
    "        out_channels : int\n",
    "            the number of output channels\n",
    "        kernel_size : int\n",
    "            the size of the convolutional kernel\n",
    "        stride_deconv : int\n",
    "            the stride of the deconvolution\n",
    "        padding : int\n",
    "            the size of padding\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sequence : nn.Sequence\n",
    "            a sequence of deconvolutional and activation modules\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride_deconv,\n",
    "                               padding=padding),\n",
    "        )\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Flattens the inputs to only 3 dimensions, preserving the sizes of the 1st and 2nd.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input data tensor of shape (dim1, dim2, *).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        flattened : torch.Tensor\n",
    "            Flattened input (dim1, dim2, dim3)\n",
    "        \"\"\"\n",
    "        return x.view(x.size(0), x.size(1), -1)\n",
    "\n",
    "\n",
    "def handle_integer_input(input, desired_len):\n",
    "    \"\"\"\n",
    "    Checks if the input is an integer or a list.\n",
    "    If an integer, it is replicated the number of  desired times\n",
    "    If a tuple, the tuple is returned as it is\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input : int, tuple\n",
    "        The input can be either a tuple of parameters or a single parameter to be replicated\n",
    "    desired_len : int\n",
    "        The length of the desired list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    input : tuple[int]\n",
    "        a tuple of parameters which has the proper length.\n",
    "    \"\"\"\n",
    "    if type(input) is int:\n",
    "        return (input,) * desired_len\n",
    "    elif type(input) is tuple:\n",
    "        if len(input) != desired_len:\n",
    "            raise AssertionError(\"The sizes of the parameters for the CNN conceptizer do not match.\"\n",
    "                                 f\"Expected '{desired_len}', but got '{len(input)}'\")\n",
    "        else:\n",
    "            return input\n",
    "    else:\n",
    "        raise TypeError(f\"Wrong type of the parameters. Expected tuple or int but got '{type(input)}'\")\n",
    "\n",
    "\n",
    "class ScalarMapping(nn.Module):\n",
    "    def __init__(self, conv_block_size):\n",
    "        \"\"\"\n",
    "        Module that maps each filter of a convolutional block to a scalar value\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        conv_block_size : tuple (int iterable)\n",
    "            Specifies the size of the input convolutional block: (NUM_CHANNELS, FILTER_HEIGHT, FILTER_WIDTH)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_filters, self.filter_height, self.filter_width = conv_block_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(self.num_filters):\n",
    "            self.layers.append(nn.Linear(self.filter_height * self.filter_width, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Reduces a 3D convolutional block to a 1D vector by mapping each 2D filter to a scalar value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input data tensor of shape (BATCH, CHANNELS, HEIGHT, WIDTH).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mapped : torch.Tensor\n",
    "            Reduced input (BATCH, CHANNELS, 1)\n",
    "        \"\"\"\n",
    "        x = x.view(-1, self.num_filters, self.filter_height * self.filter_width)\n",
    "        mappings = []\n",
    "        for f, layer in enumerate(self.layers):\n",
    "            mappings.append(layer(x[:, [f], :]))\n",
    "        return torch.cat(mappings, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8785053",
   "metadata": {},
   "source": [
    "#### Parameterizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3dc8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearParameterizer(nn.Module):\n",
    "    def __init__(self, num_concepts, num_classes, hidden_sizes=(10, 5, 5, 10), dropout=0.5, **kwargs):\n",
    "        \"\"\"Parameterizer for compas dataset.\n",
    "        \n",
    "        Solely consists of fully connected modules.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_concepts : int\n",
    "            Number of concepts that should be parameterized (for which the relevances should be determined).\n",
    "        num_classes : int\n",
    "            Number of classes that should be distinguished by the classifier.\n",
    "        hidden_sizes : iterable of int\n",
    "            Indicates the size of each layer in the network. The first element corresponds to\n",
    "            the number of input features.\n",
    "        dropout : float\n",
    "            Indicates the dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_concepts = num_concepts\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout = dropout\n",
    "        layers = []\n",
    "        for h, h_next in zip(hidden_sizes, hidden_sizes[1:]):\n",
    "            layers.append(nn.Linear(h, h_next))\n",
    "            layers.append(nn.Dropout(self.dropout))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.pop()\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of compas parameterizer.\n",
    "\n",
    "        Computes relevance parameters theta.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input data tensor of shape (BATCH, *). Only restriction on the shape is that\n",
    "            the first dimension should correspond to the batch size.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        parameters : torch.Tensor\n",
    "            Relevance scores associated with concepts. Of shape (BATCH, NUM_CONCEPTS, NUM_CLASSES)\n",
    "        \"\"\"\n",
    "        return self.layers(x).view(x.size(0), self.num_concepts, self.num_classes)\n",
    "\n",
    "\n",
    "class ConvParameterizer(nn.Module):\n",
    "    def __init__(self, num_concepts, num_classes, cl_sizes=(1, 10, 20), kernel_size=5, hidden_sizes=(10, 5, 5, 10), dropout=0.5,\n",
    "                 **kwargs):\n",
    "        \"\"\"Parameterizer for MNIST dataset.\n",
    "\n",
    "        Consists of convolutional as well as fully connected modules.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_concepts : int\n",
    "            Number of concepts that should be parameterized (for which the relevances should be determined).\n",
    "        num_classes : int\n",
    "            Number of classes that should be distinguished by the classifier.\n",
    "        cl_sizes : iterable of int\n",
    "            Indicates the number of kernels of each convolutional layer in the network. The first element corresponds to\n",
    "            the number of input channels.\n",
    "        kernel_size : int\n",
    "            Indicates the size of the kernel window for the convolutional layers.\n",
    "        hidden_sizes : iterable of int\n",
    "            Indicates the size of each fully connected layer in the network. The first element corresponds to\n",
    "            the number of input features. The last element must be equal to the number of concepts multiplied with the\n",
    "            number of output classes.\n",
    "        dropout : float\n",
    "            Indicates the dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_concepts = num_concepts\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.cl_sizes = cl_sizes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        cl_layers = []\n",
    "        for h, h_next in zip(cl_sizes, cl_sizes[1:]):\n",
    "            cl_layers.append(nn.Conv2d(h, h_next, kernel_size=self.kernel_size))\n",
    "            # TODO: maybe adaptable parameters for pool kernel size and stride\n",
    "            cl_layers.append(nn.AvgPool2d(2, stride=2))\n",
    "            cl_layers.append(nn.ReLU())\n",
    "        # dropout before maxpool\n",
    "        cl_layers.insert(-2, nn.Dropout2d(self.dropout))\n",
    "        self.cl_layers = nn.Sequential(*cl_layers)\n",
    "\n",
    "        fc_layers = []\n",
    "        for h, h_next in zip(hidden_sizes, hidden_sizes[1:]):\n",
    "            fc_layers.append(nn.Linear(h, h_next))\n",
    "            fc_layers.append(nn.Dropout(self.dropout))\n",
    "            fc_layers.append(nn.ReLU())\n",
    "        fc_layers.pop()\n",
    "        fc_layers.append(nn.Tanh())\n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of MNIST parameterizer.\n",
    "\n",
    "        Computes relevance parameters theta.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input data tensor of shape (BATCH, *). Only restriction on the shape is that\n",
    "            the first dimension should correspond to the batch size.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        parameters : torch.Tensor\n",
    "            Relevance scores associated with concepts. Of shape (BATCH, NUM_CONCEPTS, NUM_CLASSES)\n",
    "        \"\"\"\n",
    "        cl_output = self.cl_layers(x)\n",
    "        flattened = cl_output.view(x.size(0), -1)\n",
    "        return self.fc_layers(flattened).view(-1, self.num_concepts, self.num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a98ccf",
   "metadata": {},
   "source": [
    "#### Aggregators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13fe1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumAggregator(nn.Module):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"Basic Sum Aggregator that joins the concepts and relevances by summing their products.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, concepts, relevances):\n",
    "        \"\"\"Forward pass of Sum Aggregator.\n",
    "\n",
    "        Aggregates concepts and relevances and returns the predictions for each class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        concepts : torch.Tensor\n",
    "            Contains the output of the conceptizer with shape (BATCH, NUM_CONCEPTS, DIM_CONCEPT=1).\n",
    "        relevances : torch.Tensor\n",
    "            Contains the output of the parameterizer with shape (BATCH, NUM_CONCEPTS, NUM_CLASSES).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        class_predictions : torch.Tensor\n",
    "            Predictions for each class. Shape - (BATCH, NUM_CLASSES)\n",
    "            \n",
    "        \"\"\"\n",
    "        aggregated = torch.bmm(relevances.permute(0, 2, 1), concepts).squeeze(-1)\n",
    "        return F.log_softmax(aggregated, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d00d1d",
   "metadata": {},
   "source": [
    "#### SENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a5315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SENN(nn.Module):\n",
    "    def __init__(self, conceptizer, parameterizer, aggregator):\n",
    "        \"\"\"Represents a Self Explaining Neural Network (SENN).\n",
    "        (https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks)\n",
    "\n",
    "        A SENN model is a neural network made explainable by design. It is made out of several submodules:\n",
    "            - conceptizer\n",
    "                Model that encodes raw input into interpretable feature representations of\n",
    "                that input. These feature representations are called concepts.\n",
    "            - parameterizer\n",
    "                Model that computes the parameters theta from given the input. Each concept\n",
    "                has with it associated one theta, which acts as a ``relevance score'' for that concept.\n",
    "            - aggregator\n",
    "                Predictions are made with a function g(theta_1 * h_1, ..., theta_n * h_n), where\n",
    "                h_i represents concept i. The aggregator defines the function g, i.e. how each\n",
    "                concept with its relevance score is combined into a prediction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        conceptizer : Pytorch Module\n",
    "            Model that encodes raw input into interpretable feature representations of\n",
    "            that input. These feature representations are called concepts.\n",
    "\n",
    "        parameterizer : Pytorch Module\n",
    "            Model that computes the parameters theta from given the input. Each concept\n",
    "            has with it associated one theta, which acts as a ``relevance score'' for that concept.\n",
    "\n",
    "        aggregator : Pytorch Module\n",
    "            Predictions are made with a function g(theta_1 * h_1, ..., theta_n * h_n), where\n",
    "            h_i represents concept i. The aggregator defines the function g, i.e. how each\n",
    "            concept with its relevance score is combined into a prediction.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conceptizer = conceptizer\n",
    "        self.parameterizer = parameterizer\n",
    "        self.aggregator = aggregator\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of SENN module.\n",
    "        \n",
    "        In the forward pass, concepts and their reconstructions are created from the input x.\n",
    "        The relevance parameters theta are also computed.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input data tensor of shape (BATCH, *). Only restriction on the shape is that\n",
    "            the first dimension should correspond to the batch size.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : torch.Tensor\n",
    "            Predictions generated by model. Of shape (BATCH, *).\n",
    "            \n",
    "        explanations : tuple\n",
    "            Model explanations given by a tuple (concepts, relevances).\n",
    "\n",
    "            concepts : torch.Tensor\n",
    "                Interpretable feature representations of input. Of shape (NUM_CONCEPTS, *).\n",
    "\n",
    "            parameters : torch.Tensor\n",
    "                Relevance scores associated with concepts. Of shape (NUM_CONCEPTS, *)\n",
    "        \"\"\"\n",
    "        concepts, recon_x = self.conceptizer(x)\n",
    "        relevances = self.parameterizer(x)\n",
    "        predictions = self.aggregator(concepts, relevances)\n",
    "        explanations = (concepts, relevances)\n",
    "        return predictions, explanations, recon_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c4d55b",
   "metadata": {},
   "source": [
    "#### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d08f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_robustness_loss(x, aggregates, concepts, relevances):\n",
    "    \"\"\"Computes Robustness Loss for MNIST data\n",
    "    \n",
    "    Formulated by Alvarez-Melis & Jaakkola (2018)\n",
    "    [https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks.pdf]\n",
    "    The loss formulation is specific to the data format\n",
    "    The concept dimension is always 1 for this project by design\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x            : torch.tensor\n",
    "                 Input as (batch_size x num_features)\n",
    "    aggregates   : torch.tensor\n",
    "                 Aggregates from SENN as (batch_size x num_classes x concept_dim)\n",
    "    concepts     : torch.tensor\n",
    "                 Concepts from Conceptizer as (batch_size x num_concepts x concept_dim)\n",
    "    relevances   : torch.tensor\n",
    "                 Relevances from Parameterizer as (batch_size x num_concepts x num_classes)\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    robustness_loss  : torch.tensor\n",
    "        Robustness loss as frobenius norm of (batch_size x num_classes x num_features)\n",
    "    \"\"\"\n",
    "    # concept_dim is always 1\n",
    "    concepts = concepts.squeeze(-1)\n",
    "    aggregates = aggregates.squeeze(-1)\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    num_concepts = concepts.size(1)\n",
    "    num_classes = aggregates.size(1)\n",
    "\n",
    "    # Jacobian of aggregates wrt x\n",
    "    jacobians = []\n",
    "    for i in range(num_classes):\n",
    "        grad_tensor = torch.zeros(batch_size, num_classes).to(x.device)\n",
    "        grad_tensor[:, i] = 1.\n",
    "        j_yx = torch.autograd.grad(outputs=aggregates, inputs=x, \\\n",
    "                                   grad_outputs=grad_tensor, create_graph=True, only_inputs=True)[0]\n",
    "        # bs x 1 x 28 x 28 -> bs x 784 x 1\n",
    "        jacobians.append(j_yx.view(batch_size, -1).unsqueeze(-1))\n",
    "    # bs x num_features x num_classes (bs x 784 x 10)\n",
    "    J_yx = torch.cat(jacobians, dim=2)\n",
    "\n",
    "    # Jacobian of concepts wrt x\n",
    "    jacobians = []\n",
    "    for i in range(num_concepts):\n",
    "        grad_tensor = torch.zeros(batch_size, num_concepts).to(x.device)\n",
    "        grad_tensor[:, i] = 1.\n",
    "        j_hx = torch.autograd.grad(outputs=concepts, inputs=x, \\\n",
    "                                   grad_outputs=grad_tensor, create_graph=True, only_inputs=True)[0]\n",
    "        # bs x 1 x 28 x 28 -> bs x 784 x 1\n",
    "        jacobians.append(j_hx.view(batch_size, -1).unsqueeze(-1))\n",
    "    # bs x num_features x num_concepts\n",
    "    J_hx = torch.cat(jacobians, dim=2)\n",
    "\n",
    "    # bs x num_features x num_classes\n",
    "    robustness_loss = J_yx - torch.bmm(J_hx, relevances)\n",
    "\n",
    "    return robustness_loss.norm(p='fro')\n",
    "\n",
    "\n",
    "def BVAE_loss(x, x_hat, z_mean, z_logvar):\n",
    "    \"\"\" Calculate Beta-VAE loss as in [1]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.tensor\n",
    "        input data to the Beta-VAE\n",
    "\n",
    "    x_hat : torch.tensor\n",
    "        input data reconstructed by the Beta-VAE\n",
    "\n",
    "    z_mean : torch.tensor\n",
    "        mean of the latent distribution of shape\n",
    "        (batch_size, latent_dim)\n",
    "\n",
    "    z_logvar : torch.tensor\n",
    "        diagonal log variance of the latent distribution of shape\n",
    "        (batch_size, latent_dim)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : torch.tensor\n",
    "        loss as a rank-0 tensor calculated as:\n",
    "        reconstruction_loss + beta * KL_divergence_loss\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        [1] Higgins, Irina, et al. \"beta-vae: Learning basic visual concepts with\n",
    "        a constrained variational framework.\" (2016).\n",
    "    \"\"\"\n",
    "    # recon_loss = F.binary_cross_entropy(x_hat, x.detach(), reduction=\"mean\")\n",
    "    recon_loss = F.mse_loss(x_hat, x.detach(), reduction=\"mean\")\n",
    "    kl_loss = kl_div(z_mean, z_logvar)\n",
    "    return recon_loss, kl_loss\n",
    "\n",
    "def mse_l1_sparsity(x, x_hat, concepts, sparsity_reg):\n",
    "    \"\"\"Sum of Mean Squared Error and L1 norm weighted by sparsity regularization parameter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.tensor\n",
    "        Input data to the encoder.\n",
    "    x_hat : torch.tensor\n",
    "        Reconstructed input by the decoder.\n",
    "    concepts : torch.Tensor\n",
    "        Concept (latent code) activations.\n",
    "    sparsity_reg : float\n",
    "        Regularizer (xi) for the sparsity term.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : torch.tensor\n",
    "        Concept loss\n",
    "    \"\"\"\n",
    "    return F.mse_loss(x_hat, x.detach()) + sparsity_reg * torch.abs(concepts).sum()\n",
    "\n",
    "\n",
    "def kl_div(mean, logvar):\n",
    "    \"\"\"Computes KL Divergence between a given normal distribution\n",
    "    and a standard normal distribution\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : torch.tensor\n",
    "        mean of the normal distribution of shape (batch_size x latent_dim)\n",
    "\n",
    "    logvar : torch.tensor\n",
    "        diagonal log variance of the normal distribution of shape (batch_size x latent_dim)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : torch.tensor\n",
    "        KL Divergence loss computed in a closed form solution\n",
    "    \"\"\"\n",
    "    batch_loss = 0.5 * (mean.pow(2) + logvar.exp() - logvar - 1).mean(dim=0)\n",
    "    loss = batch_loss.sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def zero_loss(*args, **kwargs):\n",
    "    \"\"\"Dummy loss that always returns zero.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : list\n",
    "            Can take any number of positional arguments (without using them).\n",
    "        kwargs : dict\n",
    "            Can take any number of keyword arguments (without using them).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.tensor\n",
    "            torch.tensor(0)\n",
    "        \"\"\"\n",
    "    return torch.tensor(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25096d79",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e27217ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "class Config:\n",
    "    device = get_device()\n",
    "    batch_size = 128\n",
    "    epochs = 10\n",
    "    lr = 1e-4\n",
    "    image_size = 256\n",
    "    num_classes = 2\n",
    "    num_concepts = 5\n",
    "    concept_dim = 1\n",
    "    sparsity_reg = 1e-3\n",
    "    robust_reg = 1.0\n",
    "    concept_reg = 1.0\n",
    "    save_path = './results'\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d162054",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((config.image_size, config.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97a9ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AnimalsDataset(\n",
    "    train_df, num_labels, transform\n",
    ")\n",
    "test_dataset = AnimalsDataset(\n",
    "    test_df, num_labels, transform\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94ab12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptizer = ConvConceptizer(\n",
    "    image_size=config.image_size,\n",
    "    num_concepts=config.num_concepts,\n",
    "    concept_dim=config.concept_dim,\n",
    "    image_channels=3\n",
    ")\n",
    "parameterizer = ConvParameterizer(\n",
    "    num_concepts=config.num_concepts,\n",
    "    num_classes=config.num_classes,\n",
    "    cl_sizes=(3, 10, 20),\n",
    "    hidden_sizes=(74420, 512, config.num_concepts * config.num_classes)\n",
    "\n",
    ")\n",
    "aggregator = SumAggregator(num_classes=config.num_classes)\n",
    "\n",
    "model = SENN(conceptizer, parameterizer, aggregator).to(config.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d295e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainHistory:\n",
    "    train_losses: list[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, config, train_loader):\n",
    "    os.makedirs(config.save_path, exist_ok=True)\n",
    "    model.train()\n",
    "    for epoch in range(config.epochs):\n",
    "        total_loss = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "        for images, labels in loop:\n",
    "            images = images.to(config.device)\n",
    "            labels = labels.to(config.device)\n",
    "            images.requires_grad_(True)\n",
    "\n",
    "            outputs, (concepts, relevances), recons = model(images)\n",
    "\n",
    "            labels = labels.argmax(dim=1)\n",
    "            classification_loss = F.nll_loss(outputs, labels)\n",
    "            concept_loss = mse_l1_sparsity(images, recons, concepts, config.sparsity_reg)\n",
    "            robustness_loss = mnist_robustness_loss(images, outputs, concepts, relevances)\n",
    "        \n",
    "\n",
    "            loss = classification_loss + config.concept_reg * concept_loss + config.robust_reg * robustness_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        torch.save(model.state_dict(), os.path.join(config.save_path, f\"model_epoch_{epoch+1}.pth\"))\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88f4613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 47/47 [06:29<00:00,  8.28s/it, loss=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Avg Loss: 1.0853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 47/47 [12:22<00:00, 15.79s/it, loss=0.912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Avg Loss: 0.9460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 47/47 [06:30<00:00,  8.32s/it, loss=0.815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Avg Loss: 0.8564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  21%|██▏       | 10/47 [01:33<05:44,  9.31s/it, loss=0.801]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, config, train_loader)\u001b[39m\n\u001b[32m     20\u001b[39m loss = classification_loss + config.concept_reg * concept_loss + config.robust_reg * robustness_loss\n\u001b[32m     21\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m optimizer.step()\n\u001b[32m     25\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ps/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ps/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ps/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_model(model, config, train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
